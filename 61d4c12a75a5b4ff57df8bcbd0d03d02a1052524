{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "13f6a112_bb53400e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1129564
      },
      "writtenOn": "2021-03-23T03:27:00Z",
      "side": 1,
      "message": "Paul, can you take a look at this patch?",
      "revId": "61d4c12a75a5b4ff57df8bcbd0d03d02a1052524",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "494b0765_7479acd3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1000899
      },
      "writtenOn": "2021-03-23T14:14:37Z",
      "side": 1,
      "message": "I think this looks OK though I intend ultimately to present most of the controls as a factor that adjusts the behavior either side of the current default behavior, rather then the raw value. Hence for example I now have the sr_diff_factor field instead of the sr_diff_part. As such we may want to change names to reflect this as we change them over to this form. We will also need to do the RD parameters but again I am looking to work out a better way of presenting that and hopefully reduce it to one factor each for inter and key frames as the current tweaking of individual multipliers at specific Q thresholds is likely going to cause problems and over-fitting.\n\nFor now lets use this to test the process but we may need to tweak the names, value ranges etc later.\n\n",
      "revId": "61d4c12a75a5b4ff57df8bcbd0d03d02a1052524",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7cac857c_13aa2a1a",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1000899
      },
      "writtenOn": "2021-03-23T16:18:28Z",
      "side": 1,
      "message": "Just thinking further here. If we present an interface where all parameters are adjusted in a range logically either side of 1.0 (default behavior) then do we need a rational. Couldn\u0027t we just use an integer and convert in a defined way internally. For example 100 \u003d 1.0 \u003d default behavior. 50 \u003d 0.5, 200 \u003d 2.0 etc. It might make range checking etc easier. What do you think Cheng?",
      "parentUuid": "494b0765_7479acd3",
      "revId": "61d4c12a75a5b4ff57df8bcbd0d03d02a1052524",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "e70adf57_546ab611",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1129564
      },
      "writtenOn": "2021-03-24T04:20:30Z",
      "side": 1,
      "message": "I agree that setting parameters arbitrarily will be easily trapped in overfitting. I do think that having some range constraints on these RC parameters is important to improve stability.\n\nI think that with a ML model, it is likely that the metrics could be better than the current baseline. However, my concerns are two folds.\n\nFirst, we don\u0027t know whether the ML model fully explores the capacity of the RC design. It may show some metric gain over the baseline. But if it not significantly better or having problems in some cases, for example, at some bit rate range, shall we accept the ML model?\n\nSecond, we don\u0027t know how can we maintain or improve the model. ML models are not easy to interpret and evolve.\n\nFor this command line option only, I think using rational numbers makes sense. After all it is compatible with the internal convert anyway.\n\nAnd yes, names should be corrected.",
      "parentUuid": "7cac857c_13aa2a1a",
      "revId": "61d4c12a75a5b4ff57df8bcbd0d03d02a1052524",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}